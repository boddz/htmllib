#!/usr/bin/env python

"""
=======================
The HTML (TOKEN) Parser
=======================

Take generated tokens list, run validation checks and convert into nodes that represent each HTML tag.
"""

from __future__ import annotations

import lexer

from dataclasses import dataclass


@dataclass
class HTMLOpeningTagNode:
    """
    Represents an opening HTML tag.
    """
    tag_name: str
    attributes: dict
    content: str
    cursor_start: lexer.Cursor
    cursor_end: lexer.Cursor


@dataclass
class HTMLClosingTagNode:
    """
    Represents a closing HTML tag.
    """
    tag_name: str
    cursor_start: lexer.Cursor
    cursor_end: lexer.Cursor


@dataclass
class HTMLDoctypeOrCommNode:
    """
    Represents other HTML tags such as comments and doctype declarations.
    """
    text_raw: str
    cursor_start: lexer.Cursor
    cursor_end: lexer.Cursor


@dataclass
class HTMLErrorNode:
    """
    Represents an erroneous HTML tag (if validation rules fail during parse for valid tags).
    """
    message: str
    cursor: lexer.Cursor
    exception: Exception
    extra: str


class Parser:
    """
    Used to a parse stream (str) of HTML code into valid nodes/ error nodes. First uses lexer module to create tokens
    list and then runs :meth:``self.parse_tokens_to_node_list()`` to parse into a list of nodes.  

    Usage ::

        with open("basic.html", "r") as htmlfile:
            stream = htmlfile.read()

        tree = Parser(stream)

        import pprint  # More readable prints of final node list.
        pretty = pprint.PrettyPrinter(indent=4)
        pretty.pprint(tree.parse_tokens_to_node_list())

    """
    def __init__(self, html: str, /) -> None:
        self.__html_raw = html
        self.__lexer = lexer.Lexer(html)
        self.__lexed = self.__lexer.lex()
        self.__index = 0
        self.__tags_list = []

    @property
    def html_raw(self) -> str:
        """
        The original unprocessed stream of HTML code.
        """
        return self.__html_raw

    @property
    def lexer(self) -> str:
        """
        The parser's lexer object.
        """
        return self.__lexer

    @property
    def lexed(self) -> list:
        """
        The list of tokens generated by lexing the HTML code stream passed to the parser.
        """
        return self.__lexed

    @property
    def curr_token(self) -> lexer.Token:
        """
        The current token in the list.
        """
        return self.lexed[self.__index]

    @property
    def __next_token(self) -> lexer.Token:
        """
        **For internal use only**
        
        Return current token in the list and then move to the next one.
        """
        self.__index += 1 if self.__index < len(self.lexed) - 1 else 0
        return self.lexed[self.__index]

    def parse_tokens_to_node_list(self, *, debug: bool=False) -> bool | None:
        while self.curr_token.type is not lexer.TokenTypes.EOF:
            # Opened tag.
            if self.curr_token.type is lexer.TokenTypes.ANGLE_BRACKET_L:
                start_cursor = self.curr_token.cursor

                if self.__next_token.type not in [lexer.TokenTypes.ID,
                                                lexer.TokenTypes.EXCLAMATION,
                                                lexer.TokenTypes.CLOSING_SLASH]: # ERROR NODE: non-valid tag.
                    tag = HTMLErrorNode("Non-valid tag opening past angle bracket", self.curr_token.cursor, None, None)
                    self.__tags_list.append(tag)
                    continue

                tag = HTMLOpeningTagNode(self.curr_token.value, None, None, start_cursor, None)

                # Process and validate closing tags.
                if self.curr_token.type is lexer.TokenTypes.CLOSING_SLASH:
                    if self.__next_token.type is not lexer.TokenTypes.ID: # ERROR NODE: no id for closing tag.
                        tag = HTMLErrorNode("Closing tag has not ID (tag name)", self.curr_token.cursor, None, None)
                        self.__tags_list.append(tag)
                        continue
                    tag = HTMLClosingTagNode(self.curr_token.value, start_cursor, None)
                elif self.curr_token.type is lexer.TokenTypes.EXCLAMATION:
                    tag = HTMLDoctypeOrCommNode(self.curr_token.extra, start_cursor, None)

                # Process and validate attrs if it is an open tag, not a closing tag.
                if self.__next_token.type is lexer.TokenTypes.ID: 
                    attrs = {}
                    while ...:
                        if self.curr_token.type is not lexer.TokenTypes.ID: break
                        key = self.curr_token.value
                        if self.__next_token.type is not lexer.TokenTypes.ASSIGNMENT: break
                        if self.__next_token.type is not lexer.TokenTypes.QUOTE: break
                        value = self.curr_token.value
                        attrs[key] = value
                        self.__next_token
                    tag.attributes = attrs

                # Process and validate the end brace of any tag.
                if self.curr_token.type is not lexer.TokenTypes.ANGLE_BRACKET_R:  # ERROR NODE: no matching '>' to '<'.
                    tag = HTMLErrorNode("Tag has no matching '>' to finish", self.curr_token.cursor, None, None)
                    self.__tags_list.append(tag)
                    continue

                tag.cursor_end = self.curr_token.cursor
                self.__tags_list.append(tag)

            else:  # Not starting with '<' so not a tag. Continue to next token.
                self.__next_token

        return self.__tags_list


if __name__ == "__main__":
    with open("basic.html", "r") as htmlfile:
        stream = htmlfile.read()

    tree = Parser(stream)
    # lexer.pretty_print_tokens(tree.lexed)

    import pprint  # More readable, not needed as main import.
    _pretty = pprint.PrettyPrinter(indent=4)
    
    _pretty.pprint(tree.parse_tokens_to_node_list())
